task_name: train
train: true
test: false
compile: false
ckpt_path: null
seed: 12345
data:
  _target_: dlearn.data.text_datamodule.HPDataModule
  data_dir: ${paths.data_dir}
  batch_size: 800
  train_ratio: 0.7
  num_workers: 4
  pin_memory: false
model:
  _target_: dlearn.models.gpt_module.GPTLitModule
  learning_rate: 0.001
  n_embed: 16
  block_size: 16
  n_heads: 4
  drop_p: 0.2
  n_decoder_blocks: 2
callbacks:
  model_checkpoint:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    dirpath: ${paths.output_dir}
    filename: best
    monitor: val/loss
    verbose: false
    save_last: true
    save_top_k: 1
    mode: min
    auto_insert_metric_name: false
    save_weights_only: false
    every_n_train_steps: null
    train_time_interval: null
    every_n_epochs: null
    save_on_train_epoch_end: null
  early_stopping:
    _target_: lightning.pytorch.callbacks.EarlyStopping
    monitor: val/loss
    min_delta: 0.0
    patience: 100
    verbose: false
    mode: min
    strict: true
    check_finite: true
    stopping_threshold: null
    divergence_threshold: null
    check_on_train_epoch_end: null
  model_summary:
    _target_: lightning.pytorch.callbacks.RichModelSummary
    max_depth: -1
  rich_progress_bar:
    _target_: lightning.pytorch.callbacks.RichProgressBar
logger:
  aim:
    _target_: aim.pytorch_lightning.AimLogger
    repo: ${paths.log_dir}
    experiment: ${experiment_name}
    train_metric_prefix: train/
    val_metric_prefix: val/
    test_metric_prefix: test/
    system_tracking_interval: 10
    log_system_params: true
    capture_terminal_logs: false
paths:
  root_dir: .
  data_dir: ${paths.root_dir}/data/
  log_dir: ${paths.root_dir}/logs/
  ckpt_dir: ${paths.root_dir}/outputs/
  output_dir: ${hydra:runtime.output_dir}
  work_dir: ${hydra:runtime.cwd}
trainer:
  _target_: lightning.pytorch.trainer.Trainer
  default_root_dir: ${paths.output_dir}
  min_epochs: 0
  max_epochs: 1
  accelerator: gpu
  devices: 1
  check_val_every_n_epoch: 1
  deterministic: false
  log_every_n_steps: 1
  enable_model_summary: false
  gradient_clip_val: 0.5
  limit_train_batches: 100
  limit_val_batches: 100
  num_sanity_val_steps: 0
experiment_name: hp-gpt-optuna
tune: true
optimized_metric: val/loss
